#//#persistence
lagom.persistence {

  # As a rule of thumb, the number of shards should be a factor ten greater
  # than the planned maximum number of cluster nodes. Less shards than number
  # of nodes will result in that some nodes will not host any shards. Too many
  # shards will result in less efficient management of the shards, e.g.
  # rebalancing overhead, and increased latency because the coordinator is
  # involved in the routing of the first message for each shard. The value
  # must be the same on all nodes in a running cluster. It can be changed
  # after stopping all nodes in the cluster.
  max-number-of-shards = 100

  # Persistent entities saves snapshots after this number of persistent
  # events. Snapshots are used to reduce recovery times.
  # It may be configured to "off" to disable snapshots.
  snapshot-after = 100

  # A persistent entity is passivated automatically if it does not receive
  # any messages during this timeout. Passivation is performed to reduce
  # memory consumption. Objects referenced by the entity can be garbage
  # collected after passivation. Next message will activate the entity
  # again, which will recover its state from persistent storage. Set to 0
  # to disable passivation - this should only be done when the number of
  # entities is bounded and their state, sharded across the cluster, will
  # fit in memory.
  passivate-after-idle-timeout = 120s

  # Specifies that entities run on cluster nodes with a specific role.
  # If the role is not specified (or empty) all nodes in the cluster are used.
  # The entities can still be accessed from other nodes.
  run-entities-on-role = ""

  # Default timeout for PersistentEntityRef.ask replies.
  ask-timeout = 5s

  dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      fixed-pool-size = 16
    }
    throughput = 1
  }
}
#//#persistence

#//#persistence-read-side
lagom.persistence.read-side {

  # how long should we wait when retrieving the last known offset
  offset-timeout = 5s

  # Exponential backoff for failures in ReadSideProcessor
  failure-exponential-backoff {
    # minimum (initial) duration until processor is started again
    # after failure
    min = 3s

    # the exponential back-off is capped to this duration
    max = 30s

    # additional random delay is based on this factor
    random-factor = 0.2
  }

  # The amount of time that a node should wait for the global prepare callback to execute
  global-prepare-timeout = 20s

  # Specifies that the read side processors should run on cluster nodes with a specific role.
  # If the role is not specified (or empty) all nodes in the cluster are used.
  run-on-role = ""

  # The Akka dispatcher to use for read-side actors and tasks.
  use-dispatcher = "lagom.persistence.dispatcher"
}
#//#persistence-read-side

# Cluster distribution settings
lagom.persistence.cluster.distribution {

  # Each entity is pinged at this interval. Each node will ping this often, so this interval can be quite long.
  ensure-active-interval = 30s
}

akka.actor {
  serializers {
    lagom-persistence-core = "com.lightbend.lagom.internal.persistence.cluster.ClusterStartupTaskSerializer"
  }
  serialization-bindings {
    "com.lightbend.lagom.internal.persistence.cluster.ClusterStartupTaskActor$Execute$" = lagom-persistence-core
  }
  serialization-identifiers {
    # "lagom-persistence-core".hashCode
    "com.lightbend.lagom.internal.persistence.cluster.ClusterStartupTaskSerializer" = -1543173302
  }
}

